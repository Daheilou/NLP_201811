{
  "cells": [
    {
      "metadata": {
        "_uuid": "c71b0e965d7ae417a7a9d30d5ce8c79ee25b3821"
      },
      "cell_type": "markdown",
      "source": "# 《Hands-On Machine Learning with Scikit-Learn&TensorFlow》\n\n## -  Chapter 14. Recurrent Neural Networks\n\n-----"
    },
    {
      "metadata": {
        "_uuid": "868800fef5c7e2b8a4ed39f2a6389319e3e08f47"
      },
      "cell_type": "markdown",
      "source": "## 1. Basic RNNs in TensorFlow\n![20181217174613.png](https://i.loli.net/2018/12/18/5c1850b2553fc.png)\n\n![20181218103909.png](https://i.loli.net/2018/12/18/5c184fd153f78.png)\n\nWe will assume that the RNN runs over only two time steps, taking input vectors of size 3 at each time step. The\nfollowing code builds this RNN, unrolled through two time steps:\n\n1. time steps : 2 / 2 words a sentence\n2. RNN cell input vector dim: 3, each RNN cell accepts a shape of (1,3) vector\n3. hidden size: 5"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6d6692b524c6f934477ae29195b93fef3e7d3c2"
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport numpy as np\ntf.set_random_seed(1)",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "41883c3541182386562109513a9bc542b82ac93a"
      },
      "cell_type": "code",
      "source": "n_inputs = 3 # input vector dim at each time step \nn_neurons = 5 # RNN cell hidden size",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6add987d55922d92ce8d9579e08703aacd5151a0"
      },
      "cell_type": "code",
      "source": "# 2 time steps: x0, x1\nx0 = tf.placeholder(tf.float32, shape=(None,n_inputs))\nx1 = tf.placeholder(tf.float32, shape=(None,n_inputs))",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c19fa46b8f6ab23b76674809336905e938a96c7"
      },
      "cell_type": "code",
      "source": "wx = tf.Variable(tf.random_normal(shape=(n_inputs, n_neurons), dtype=tf.float32))\nwy = tf.Variable(tf.random_normal(shape=(n_neurons, n_neurons), dtype=tf.float32))\nb = tf. Variable(tf.zeros(shape=(1,n_neurons), dtype=tf.float32))\n\ny0 = tf.tanh(tf.matmul(x0,wx)+b) # shape (1,3)x(3,5)+(1,5) = (1,5)\ny1 = tf.tanh(tf.matmul(y0,wy)+tf.matmul(x1,wx)+b)\n\ninit = tf.global_variables_initializer()",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "870582e252ab1326ed0a4f4eaa60e5e5a9bddd13"
      },
      "cell_type": "code",
      "source": "# mini batch = 4,     sentence1   sentence2  sentence3  sentence4\nx0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t=0\nx1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t=1\n\n# sentence1: x0 = [0, 1, 2], x1 = [9, 8, 7]\n# sentence2: x0 = [3, 4, 5], x1 = [0, 0, 0]\n# sentence3: x0 = [6, 7, 8], x1 = [6, 5, 4]\n# sentence4: x0 = [9, 0, 1], x1 = [3, 2, 1]",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7bfe06520a0020cd80cfc4a6e505dd92de32036"
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    sess.run(init)\n    y0_val, y1_val = sess.run([y0,y1], feed_dict={x0:x0_batch, x1:x1_batch})\nprint('y0_val:\\n',y0_val)\nprint('\\ny1_val:\\n',y1_val)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "y0_val:\n [[-0.91210926 -0.97909343 -0.9963769  -0.804197    0.81554604]\n [-0.9984175  -0.99501723 -0.9999899  -0.999553    0.878944  ]\n [-0.99997276 -0.99881977 -1.         -0.99999905  0.921494  ]\n [ 0.9867835   1.          1.         -0.5567072  -0.9989798 ]]\n\ny1_val:\n [[-0.9999856   0.8339144  -0.9999802  -0.99999195 -0.97266006]\n [-0.9276461   0.8301138   0.03124463  0.9562516  -0.9378031 ]\n [-0.9995392   0.97095305 -0.99141777 -0.99612236 -0.9843221 ]\n [-0.5459391  -0.3920396   0.943575   -0.99999213  0.9344761 ]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b222e31f7339bf043b9e44f4c9934a7214cc0e81"
      },
      "cell_type": "markdown",
      "source": "The output of each sentence/sample is:\n1.  Each timestep output of sentence 1\n```\ny0 = [-0.9157372   0.9512312  -0.8827221  -0.9911751  -0.9362729 ]\ny1 = [ 0.99998605  1.          1.          0.99998194  0.93795544]\n ```\n2. Each timestep output of sentence 2\n```\ny0 = [ 0.8368062   0.99866116  0.29276526 -0.9999999  -0.99998057]\ny1 = [-0.97918355 -0.99938613  0.9646454  -0.9705091  -0.9811791 ]\n ```\n3. Each timestep output of sentence 3\n```\ny0 = [ 0.9993057   0.9999641   0.96339613 -1.         -1.        ]\ny1 = [ 0.9958529   1.          0.9999868   0.9993763   0.9095211 ]\n ```\n4. Each timestep output of sentence 4\n```\ny0 = [ 1.         -0.99971396  0.9999998  -0.9999982  -0.97780967]\ny1 = [ 1.          0.17404278  1.         -1.         -1.        ]\n ```\nEach output of one timestep is the size of (1, hidden_size) = (1, 5)\n\nThat wasn’t too hard, but of course if you want to be able to run an RNN over 100 time steps, the graph is\ngoing to be pretty big. Now let’s look at how to create the same model using TensorFlow’s RNN\noperations."
    },
    {
      "metadata": {
        "_uuid": "fd962eb6b7154b63df2476852128c32418a70798"
      },
      "cell_type": "markdown",
      "source": "## 2. Static Unrolling Through Time"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "973799bc4248ea38c0b7660f81c7e3fc51bd6082"
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b7f4626f59ae7171a22d5a1724fe7deba05d32f"
      },
      "cell_type": "code",
      "source": "n_inputs = 3 # input vector dim at each time step \nn_neurons = 5 # RNN cell hidden size\n\nx0 = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs))\nx1 = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs))\n\nbasic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\noutput_seqs, states = tf.contrib.rnn.static_rnn(cell=basic_cell, inputs=[x0,x1], dtype=tf.float32)\ny0,y1 = output_seqs",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "090d558661104f200afaef5ae5b4843275014a03"
      },
      "cell_type": "markdown",
      "source": "```\ntf.contrib.rnn.BasicRNNCell(num_units, activation=None, reuse=None, name=None, dtype=None, **kwargs)\n```\n```\ntf.contrib.rnn.static_rnn(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None)\n```\nFirst we create the input placeholders, as before. Then we create a ```BasicRNNCell()```, which you can think\nof as a factory that creates copies of the cell to build the unrolled RNN (one for each time step). \n\nThen we call ```static_rnn()```, giving it the cell factory and the input tensors, and telling it the data type of the inputs(this is used to create the **initial state matrix, which by default is full of zeros**). \n\nThe ```static_rnn()```function calls the cell factory’s``` __call__()``` function once per input, creating two copies of the cell (each containing a layer of five recurrent neurons), **with shared weights and bias terms**, and it chains them just like we did earlier. \n\nThe ```static_rnn()``` function returns two objects. \n1. The first is a Python list containing the output tensors for each time step. \n2. The second is a tensor containing the final states of the network. When you are using basic cells, the final state is simply equal to the last output"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38b082f11f75308fe05db414c0333e166fe9fcf3",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# mini batch = 4,     sentence1   sentence2  sentence3  sentence4\nx0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t=0\nx1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t=1\nwith tf.Session() as sess:\n    # sess.run(init)\n    sess.run(tf.global_variables_initializer())\n    y0_val, y1_val = sess.run([y0,y1], feed_dict={x0:x0_batch, x1:x1_batch})\nprint('y0_val:\\n',y0_val)\nprint('\\ny1_val:\\n',y1_val)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "y0_val:\n [[ 0.65068823 -0.7885722  -0.68872046 -0.579271    0.5519878 ]\n [ 0.9604823  -0.9950424  -0.9997227  -0.5949103   0.8591207 ]\n [ 0.9961673  -0.9998956  -0.9999998  -0.6101117   0.96099204]\n [-0.4338037  -0.33560485 -0.9999477  -0.36008093  0.70006984]]\n\ny1_val:\n [[ 0.9983586  -0.9998548  -1.          0.80598366  0.7580543 ]\n [ 0.8172029   0.09068511 -0.8805279   0.5421987  -0.5194732 ]\n [ 0.9914208  -0.99018264 -0.99999976  0.845585    0.12200028]\n [ 0.6654841  -0.707808   -0.99619687  0.8441219  -0.2600849 ]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "dc3fb364fe34fad0e55a40d4d4f1346e35e08a94"
      },
      "cell_type": "markdown",
      "source": "If there were 50 time steps, it would not be very convenient to have to define 50 input placeholders and 50 output tensors. \n```\nx0 = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs))\nx1 = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs))\n...\n\nx49 = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs))\n```\n\nMoreover, at execution time you would have to feed each of the 50 placeholders and\nmanipulate the 50 outputs. \n\nLet’s simplify this. The following code builds the same RNN again, but this time it takes a single input placeholder of shape ```[batch_size, n_steps, n_inputs]``` where the first dimension is the mini-batch size.  즉, put all 50 timesteps (50 words a sentence) together in one tenser/array/list."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2e54a8097d0292eaf3f1a605e5f7f3776897751"
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "8ce746ce9fd5bb2c1ffc810568b9f19eae7e03e9"
      },
      "cell_type": "code",
      "source": "n_steps = 2\n\nx = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\nx_seqs = tf.unstack(tf.transpose(x, perm=[1, 0, 2]))\nbasic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\noutput_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, \n                                                x_seqs, \n                                                dtype=tf.float32)\noutputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e91fe5196d6bd6e3cd0427ba89a3f3db6245d420"
      },
      "cell_type": "markdown",
      "source": "input placeholder of shape ```[None, n_steps, n_inputs]``` where the first dimension is the mini-batch size. \n\nThen it extracts the list of input sequences for each time step. ```X_seqs``` is a Python list of ```n_steps``` tensors of shape ```[None, n_inputs]```, where once again the first dimension is the minibatch size. \n\nTo do this, we first swap the first two dimensions using the ```transpose()``` function, so that the time steps are now the first dimension. \n\nThen we extract a Python list of tensors along the first dimension (i.e., one tensor per time step) using the ```unstack()``` function. \n\nThe next two lines are the same as before. Finally, we merge all the output tensors into a single tensor using the ```stack()``` function, and we swap the first two dimensions to get a final outputs tensor of shape ```[None, n_steps, n_neurons]``` (again the first dimension is the mini-batch size)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea1d3f2e15ad3ae0661ceed1b3ec6162374d507d"
      },
      "cell_type": "code",
      "source": "x_batch = np.array([\n# t = 0        t = 1\n[[0, 1, 2], [9, 8, 7]], # instance 0\n[[3, 4, 5], [0, 0, 0]], # instance 1\n[[6, 7, 8], [6, 5, 4]], # instance 2\n[[9, 0, 1], [3, 2, 1]], # instance 3\n])",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fec22516bc1845da770e34f7f0de5ca8fcb27e14"
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    outputs_val = sess.run(outputs, feed_dict={x:x_batch})",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "035b30d1103993b2a0f06a2b8c85d47aa8ffba97"
      },
      "cell_type": "code",
      "source": "print(outputs_val)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[[ 0.7030044   0.9178484  -0.79409844  0.1814973  -0.9564625 ]\n  [ 0.9999998   0.990394   -1.          0.9705728  -1.        ]]\n\n [[ 0.9994244   0.99556637 -0.99984914  0.7777229  -0.9999891 ]\n  [-0.16180846 -0.44066358  0.60035974 -0.58253336 -0.4148272 ]]\n\n [[ 0.99999905  0.99976957 -0.9999999   0.95586205 -1.        ]\n  [ 0.99995315  0.6604901  -0.9999651   0.6610545  -0.9999976 ]]\n\n [[ 0.9998051  -0.9945523  -0.99998325 -0.88014686 -0.9161856 ]\n  [ 0.8684468   0.8314037  -0.9810648   0.89922035 -0.99718755]]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b2c7a752dc9b676ac6b4b9a2037f8a07c49820d7"
      },
      "cell_type": "markdown",
      "source": "However, this approach still builds a graph containing one cell per time step. If there were 50 time steps, the graph would look pretty ugly. It is a bit like writing a program without ever using loops, e.g.\n```\n(Y0=f(0,X0); Y1=f(Y0, X1); Y2=f(Y1, X2); ...; Y50=f(Y49, X50))\n```\n\nWith such as large graph, you may even get out-of-memory (OOM) errors during backpropagation (especially with the limited memory of GPU cards), since it must store all tensor values during the forward pass so it can use them to compute gradients during the reverse pass."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8a8e9ba5b828742c2b19582be6a93b392c412fa"
      },
      "cell_type": "markdown",
      "source": "## 3. Dynamic Unrolling Through Time\nThe ```dynamic_rnn()``` function uses a ```while_loop()``` operation to run over the cell the appropriate number of times, and you can set swap_memory=True if you want it to swap the GPU’s memory to the CPU’s\nmemory during backpropagation to avoid OOM errors. \n\nConveniently, it also accepts a single tensor for all inputs at every time step ```(shape [None, n_steps, n_inputs])``` and it outputs a single tensor for all outputs at every time step``` (shape [None, n_steps, n_neurons])```; \nthere is no need to stack, unstack, or transpose. \n\nThe following code creates the same RNN as earlier using the ```dynamic_rnn()``` function. It’s so much nicer!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24a438b11595930389bc5730373085389534e223"
      },
      "cell_type": "code",
      "source": "tf.reset_default_graph()",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d4f832591aba3d0469f4beac48541899ae2ec1b"
      },
      "cell_type": "code",
      "source": "x = tf.placeholder(tf.float32, shape=(None, n_steps, n_inputs))\nbasic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\noutputs, states = tf.nn.dynamic_rnn(cell=basic_cell, inputs=x, dtype=tf.float32)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "52ddd2a6c24521afe9a4f43df848da08d1430251"
      },
      "cell_type": "code",
      "source": "with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    outputs_val = sess.run(outputs, feed_dict={x:x_batch})\nprint(outputs_val)",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[[ 8.3456165e-01  4.4506708e-01 -2.1010002e-01 -3.8176674e-01\n    9.5100176e-01]\n  [ 6.9185251e-01  9.9999988e-01  3.9719999e-01  2.0911363e-01\n    9.9997133e-01]]\n\n [[ 9.7582531e-01  9.9783802e-01 -4.1931435e-01 -5.0139934e-01\n    9.9969459e-01]\n  [-8.3180672e-01 -2.0288916e-01  6.9902909e-01  4.1513512e-01\n   -5.9112889e-04]]\n\n [[ 9.9668550e-01  9.9999386e-01 -5.9180784e-01 -6.0450792e-01\n    9.9999809e-01]\n  [-4.4303608e-01  9.9992198e-01  5.1096505e-01  5.6005251e-01\n    9.9670297e-01]]\n\n [[-9.9883330e-01  9.9997902e-01  9.6597314e-01  1.2116167e-02\n   -9.9381185e-01]\n  [ 3.2501018e-01  9.8728287e-01 -7.4440765e-01  2.7665448e-01\n    9.3733406e-01]]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12c221e867e8431d19b25ca08cd99026c5de27da"
      },
      "cell_type": "markdown",
      "source": "## 4. LSTM Cell"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef0b59d27939475e736e764f197637cdf6ea741e"
      },
      "cell_type": "code",
      "source": "lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77a3c033dc533811b301a66cafa6b3d6690cc5d2"
      },
      "cell_type": "code",
      "source": "lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b90d0e3205cf66204c33f04de5d4bad9933b8c2"
      },
      "cell_type": "markdown",
      "source": "1. Init a tf  tensor\n\n```python\ntf.InteractiveSession()\n\na = tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[0,1,2]],[[3,4,5],[6,7,8]]])\nprint(a.shape)\na.eval()\n```\n[out]:\n\n```\n(3, 2, 3)\narray([[[1, 2, 3],\n        [4, 5, 6]],\n\n       [[7, 8, 9],\n        [0, 1, 2]],\n\n       [[3, 4, 5],\n        [6, 7, 8]]], dtype=int32)\n```\n2. tf.transpose(a, perm=None, name='transpose', conjugate=False)\n\n change the shape by origin position idx through perm\n\n```python\nb = tf.transpose(a, perm=[1,0,2])   \nprint(b.shape)\nb.eval()\n```\n[out]:\n\n```\n(2, 3, 3)\narray([[[1, 2, 3],\n        [7, 8, 9],\n        [3, 4, 5]],\n\n       [[4, 5, 6],\n        [0, 1, 2],\n        [6, 7, 8]]], dtype=int32)\n```\n3. tf.unstack(value, num=None, axis=0, name='unstack')\n\n```python\nc = tf.unstack(b)\nfor i in c:\n    print(i.eval(),'\\n-----')\n```\n[out]:\n\n```\n[[1 2 3]\n [7 8 9]\n [3 4 5]] \n-----\n[[4 5 6]\n [0 1 2]\n [6 7 8]] \n-----\n```\n-----\n\n**All-in-one**\n```python\nx = tf.reshape(a, [-1, 3])\nx.eval()\n```\n[out]:\n\n```\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9],\n       [0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]], dtype=int32)\n```\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a084b8de5d294b4d82b673bd89220b0b8a972dd"
      },
      "cell_type": "code",
      "source": "\ntf.InteractiveSession()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3a72fd2d356dc22c92a59f60f3f7e0eec0ccbdf"
      },
      "cell_type": "code",
      "source": "a = tf.constant([[[1,2,3],[4,5,6]],[[7,8,9],[0,1,2]],[[3,4,5],[6,7,8]]])\nprint(a.shape)\na.eval()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "befa228b939f6a98d57f71349f90fa443589c336",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "b = tf.transpose(a, perm=[1,0,2])\nprint(b.shape)\nb.eval()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0390d025d906e66926e102db65ccb77078720d0a"
      },
      "cell_type": "code",
      "source": "c = tf.unstack(b)\nfor i in c:\n    print(i.eval(),'\\n-----')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4472db2dbf374629fa977bae74decc0700cf0086"
      },
      "cell_type": "code",
      "source": "x = tf.unstack(a, 2, 1)\nfor i in x:\n    print(i.eval(),'\\n-----')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1ebf9003a6e12527e4b2493c4f9bdd2ae7bc59d"
      },
      "cell_type": "code",
      "source": "x = tf.reshape(a, [-1, 3])\nx.eval()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff267957631a5a0eab5c3ba4afd5bfb25536b3e8"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90bc5a092fc45e6a7892bcb9050f93101854e11e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98c553ca1c7e2b6d8b0753f3ae00a58be846999a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1b2c3b0eaa1da31c39bdc60158587ca8b94a3ed"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}