{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "c5052a6125b8d68cdab6de7af5cd78ac4973d770"
      },
      "cell_type": "markdown",
      "source": "### Imports"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8db3f9951b298028b68d17a0e3f4c95d644f40fc"
      },
      "cell_type": "code",
      "source": "import numpy as np\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython import display\nplt.style.use('seaborn-white')",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9a0ce381557e91488f2be916996e34a05bb3f3c8"
      },
      "cell_type": "markdown",
      "source": "### Read and process data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3940c1a7b31fb1b5c540a51464558eedf71ac7be"
      },
      "cell_type": "code",
      "source": "data = open('../input/kafka.txt', 'r').read()",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6219d6503e9c55719050b4512d1c87622b0b5d5a"
      },
      "cell_type": "markdown",
      "source": "Process data and calculate indexes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "718efd341686bf30b708f5b84232d73952fbdcd3"
      },
      "cell_type": "code",
      "source": "chars = list(set(data))\ndata_size, X_size = len(data), len(chars)\nprint(\"data has %d characters, %d unique\" % (data_size, X_size))\nchar_to_idx = {ch:i for i,ch in enumerate(chars)}\nidx_to_char = {i:ch for i,ch in enumerate(chars)}",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "data has 118561 characters, 63 unique\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c9104de103c219d9e663ffbd53ca867ac9090ba3"
      },
      "cell_type": "markdown",
      "source": "### Constants and Hyperparameters"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37dbba45d3e6667294b81e19dac866f49f64e310"
      },
      "cell_type": "code",
      "source": "H_size = 100 # Size of the hidden layer\nT_steps = 25 # Number of time steps (length of the sequence) used for training\nlearning_rate = 1e-1 # Learning rate\nweight_sd = 0.1 # Standard deviation of weights for initialization\nz_size = H_size + X_size # Size of concatenate(H, X) vector",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72998041f58160e430121d8b5f3a65d6d72d2ab6"
      },
      "cell_type": "markdown",
      "source": "### Activation Functions and Derivatives\n\n#### Sigmoid\n\n\\begin{align}\n\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n\\end{align}\n\n#### Tanh\n\n\\begin{align}\n\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n\\end{align}"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9698f4eada2fce50044265b0f8aec76f8358bc2c"
      },
      "cell_type": "code",
      "source": "def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef dsigmoid(y):\n    return y * (1 - y)\n\n\ndef tanh(x):\n    return np.tanh(x)\n\n\ndef dtanh(y):\n    return 1 - y * y",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9098658bd734b46726f7f475f74a9a19778d415f"
      },
      "cell_type": "markdown",
      "source": "### Parameters"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bca9c96a223746ec5328d5142d0593f387d3231b"
      },
      "cell_type": "code",
      "source": "class Param:\n    def __init__(self, name, value):\n        self.name = name\n        self.v = value #parameter value\n        self.d = np.zeros_like(value) #derivative\n        self.m = np.zeros_like(value) #momentum for AdaGrad",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee78f486a511a636e7449772ac5a5af4fa0086fc"
      },
      "cell_type": "markdown",
      "source": "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n\nBiases are initialized to zeros."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb092577673049489763a56bc7d2420736e9d2ee"
      },
      "cell_type": "code",
      "source": "class Parameters:\n    def __init__(self):\n        self.W_f = Param('W_f', \n                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n        self.b_f = Param('b_f',\n                         np.zeros((H_size, 1)))\n\n        self.W_i = Param('W_i',\n                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n        self.b_i = Param('b_i',\n                         np.zeros((H_size, 1)))\n\n        self.W_C = Param('W_C',\n                         np.random.randn(H_size, z_size) * weight_sd)\n        self.b_C = Param('b_C',\n                         np.zeros((H_size, 1)))\n\n        self.W_o = Param('W_o',\n                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n        self.b_o = Param('b_o',\n                         np.zeros((H_size, 1)))\n\n        #For final layer to predict the next character\n        self.W_v = Param('W_v',\n                         np.random.randn(X_size, H_size) * weight_sd)\n        self.b_v = Param('b_v',\n                         np.zeros((X_size, 1)))\n        \n    def all(self):\n        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n        \nparameters = Parameters()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2ba0527fdba17069bd8d42d6b712c5253e5086c5"
      },
      "cell_type": "markdown",
      "source": "### Forward pass\n\n![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n\n*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n\n#### Concatenation of $h_{t-1}$ and $x_t$\n\\begin{align}\nz & = [h_{t-1}, x_t] \\\\\n\\end{align}\n\n#### LSTM functions\n\\begin{align}\nf_t & = \\sigma(W_f \\cdot z + b_f) \\\\\ni_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\nC_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\no_t & = \\sigma(W_o \\cdot z + b_t) \\\\\nh_t &= o_t * tanh(C_t) \\\\\n\\end{align}\n\n#### Logits\n\\begin{align}\nv_t &= W_v \\cdot h_t + b_v \\\\\n\\end{align}\n\n#### Softmax\n\\begin{align}\n\\hat{y_t} &= \\text{softmax}(v_t)\n\\end{align}\n\n$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd689bb10578f410a042ae6b4466790798c65e22"
      },
      "cell_type": "code",
      "source": "def forward(x, h_prev, C_prev, p = parameters):\n    assert x.shape == (X_size, 1)\n    assert h_prev.shape == (H_size, 1)\n    assert C_prev.shape == (H_size, 1)\n    \n    z = np.row_stack((h_prev, x))\n    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n\n    C = f * C_prev + i * C_bar\n    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n    h = o * tanh(C)\n\n    v = np.dot(p.W_v.v, h) + p.b_v.v\n    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n\n    return z, f, i, C_bar, C, o, h, v, y",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c42ad893d690a9da881e8caf3470aaf884fcde19"
      },
      "cell_type": "markdown",
      "source": "### Backward pass\n\n#### Loss\n\n\\begin{align}\nL_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\nL &= L_1 \\\\\n\\end{align}\n\n#### Gradients\n\n\\begin{align}\ndv_t &= \\hat{y_t} - y_t \\\\\ndh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\ndo_t &= dh_t * \\text{tanh}(C_t) \\\\\ndC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\nd\\bar{C}_t &= dC_t * i_t \\\\\ndi_t &= dC_t * \\bar{C}_t \\\\\ndf_t &= dC_t * C_{t-1} \\\\\n\\\\\ndf'_t &= f_t * (1 - f_t) * df_t \\\\\ndi'_t &= i_t * (1 - i_t) * di_t \\\\\nd\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\ndo'_t &= o_t * (1 - o_t) * do_t \\\\\ndz_t &= W_f^T \\cdot df'_t \\\\\n     &+ W_i^T \\cdot di_t \\\\\n     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n     &+ W_o^T \\cdot do_t \\\\\n\\\\\n[dh'_{t-1}, dx_t] &= dz_t \\\\\ndC'_t &= f_t * dC_t\n\\end{align}\n\n* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n* All other derivatives are of $L$\n* `target` is target character index $y_t$\n* `dh_next` is $dh'_{t}$ (size H x 1)\n* `dC_next` is $dC'_{t}$ (size H x 1)\n* `C_prev` is $C_{t-1}$ (size H x 1)\n* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n* *Returns* $dh_t$ and $dC_t$\n\n#### Model parameter gradients\n\n\\begin{align}\ndW_v &= dv_t \\cdot h_t^T \\\\\ndb_v &= dv_t \\\\\n\\\\\ndW_f &= df'_t \\cdot z^T \\\\\ndb_f &= df'_t \\\\\n\\\\\ndW_i &= di'_t \\cdot z^T \\\\\ndb_i &= di'_t \\\\\n\\\\\ndW_C &= d\\bar{C}'_t \\cdot z^T \\\\\ndb_C &= d\\bar{C}'_t \\\\\n\\\\\ndW_o &= do'_t \\cdot z^T \\\\\ndb_o &= do'_t \\\\\n\\\\\n\\end{align}"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb83bffc827f4c66a214c78d3bb2ea6033bd628d"
      },
      "cell_type": "code",
      "source": "def backward(target, dh_next, dC_next, C_prev,\n             z, f, i, C_bar, C, o, h, v, y,\n             p = parameters):\n    \n    assert z.shape == (X_size + H_size, 1)\n    assert v.shape == (X_size, 1)\n    assert y.shape == (X_size, 1)\n    \n    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n        assert param.shape == (H_size, 1)\n        \n    dv = np.copy(y)\n    dv[target] -= 1\n\n    p.W_v.d += np.dot(dv, h.T)\n    p.b_v.d += dv\n\n    dh = np.dot(p.W_v.v.T, dv)        \n    dh += dh_next\n    do = dh * tanh(C)\n    do = dsigmoid(o) * do\n    p.W_o.d += np.dot(do, z.T)\n    p.b_o.d += do\n\n    dC = np.copy(dC_next)\n    dC += dh * o * dtanh(tanh(C))\n    dC_bar = dC * i\n    dC_bar = dtanh(C_bar) * dC_bar\n    p.W_C.d += np.dot(dC_bar, z.T)\n    p.b_C.d += dC_bar\n\n    di = dC * C_bar\n    di = dsigmoid(i) * di\n    p.W_i.d += np.dot(di, z.T)\n    p.b_i.d += di\n\n    df = dC * C_prev\n    df = dsigmoid(f) * df\n    p.W_f.d += np.dot(df, z.T)\n    p.b_f.d += df\n\n    dz = (np.dot(p.W_f.v.T, df)\n         + np.dot(p.W_i.v.T, di)\n         + np.dot(p.W_C.v.T, dC_bar)\n         + np.dot(p.W_o.v.T, do))\n    dh_prev = dz[:H_size, :]\n    dC_prev = f * dC\n    \n    return dh_prev, dC_prev",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2f090dbe334bcbff31f18b3de6ea60a5ec62404a"
      },
      "cell_type": "markdown",
      "source": "### Forward Backward Pass"
    },
    {
      "metadata": {
        "_uuid": "95fbe5f8970f90f2433b8d55906d132ccc61a20b"
      },
      "cell_type": "markdown",
      "source": "Clear gradients before each backward pass"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "539c988ea7afb3038973da4b2a8f8c30e8293ff3"
      },
      "cell_type": "code",
      "source": "def clear_gradients(params = parameters):\n    for p in params.all():\n        p.d.fill(0)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "03efb229bfcf247638016c8984663d275fd561c3"
      },
      "cell_type": "markdown",
      "source": "Clip gradients to mitigate exploding gradients"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0877f12d57be14f7586c6faca7c9efc9ede26b19"
      },
      "cell_type": "code",
      "source": "def clip_gradients(params = parameters):\n    for p in params.all():\n        np.clip(p.d, -1, 1, out=p.d)",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d5b5db97d66de25bd24aeef954158b51bf8097fd"
      },
      "cell_type": "markdown",
      "source": "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n\n* `input`, `target` are list of integers, with character indexes.\n* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n* *Returns* loss, final $h_T$ and $C_T$"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f87b8eb0e523e334332ca439beecff0aa67b9dd"
      },
      "cell_type": "code",
      "source": "def forward_backward(inputs, targets, h_prev, C_prev):\n    global paramters\n    \n    # To store the values for each time step\n    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n    v_s, y_s =  {}, {}\n    \n    # Values at t - 1\n    h_s[-1] = np.copy(h_prev)\n    C_s[-1] = np.copy(C_prev)\n    \n    loss = 0\n    # Loop through time steps\n    assert len(inputs) == T_steps\n    for t in range(len(inputs)):\n        x_s[t] = np.zeros((X_size, 1))\n        x_s[t][inputs[t]] = 1 # Input character\n        \n        (z_s[t], f_s[t], i_s[t],\n        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n        v_s[t], y_s[t]) = \\\n            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n            \n        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n        \n    clear_gradients()\n\n    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n\n    for t in reversed(range(len(inputs))):\n        # Backward pass\n        dh_next, dC_next = \\\n            backward(target = targets[t], dh_next = dh_next,\n                     dC_next = dC_next, C_prev = C_s[t-1],\n                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n                     y = y_s[t])\n\n    clip_gradients()\n        \n    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6b7cbd8e6e7e42d8e2ee46c901bc4ae62397723d"
      },
      "cell_type": "markdown",
      "source": "### Sample the next character"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e72768ffd6401ffba066c4aab7b3781d7ea47ba3"
      },
      "cell_type": "code",
      "source": "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n    x = np.zeros((X_size, 1))\n    x[first_char_idx] = 1\n\n    h = h_prev\n    C = C_prev\n\n    indexes = []\n    \n    for t in range(sentence_length):\n        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n        idx = np.random.choice(range(X_size), p=p.ravel())\n        x = np.zeros((X_size, 1))\n        x[idx] = 1\n        indexes.append(idx)\n\n    return indexes",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b0d476af9553ca622e37ce32fcf58c6ab33fbfd"
      },
      "cell_type": "markdown",
      "source": "## Training (Adagrad)"
    },
    {
      "metadata": {
        "_uuid": "6955c22a5d331abe0b9b81df94ba8bc569e62deb"
      },
      "cell_type": "markdown",
      "source": "Update the graph and display a sample output"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d1887c544e81376719b3b0cb5d6c0f4592eff20"
      },
      "cell_type": "code",
      "source": "def update_status(inputs, h_prev, C_prev):\n    #initialized later\n    global plot_iter, plot_loss\n    global smooth_loss\n    \n    # Get predictions for 200 letters with current model\n\n    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n\n    # Clear and plot\n    plt.plot(plot_iter, plot_loss)\n    display.clear_output(wait=True)\n    plt.show()\n\n    #Print prediction and loss\n    print(\"----\\n %s \\n----\" % (txt, ))\n    print(\"iter %d, loss %f\" % (iteration, smooth_loss))",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4c248449ab68c8f4c93d515c0cd442f4008144c"
      },
      "cell_type": "markdown",
      "source": "Update parameters\n\n\\begin{align}\n\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\nd\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n\\end{align}"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45af691becbefd508f8025eb89129eb8af3abf8d"
      },
      "cell_type": "code",
      "source": "def update_paramters(params = parameters):\n    for p in params.all():\n        p.m += p.d * p.d # Calculate sum of gradients\n        #print(learning_rate * dparam)\n        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9edfe15fd2b19fd75f82d8f9615a4fa0d7dff06b"
      },
      "cell_type": "markdown",
      "source": "To delay the keyboard interrupt to prevent the training \nfrom stopping in the middle of an iteration "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60924e54ac81836b4095be81caf2b03bff80a87b"
      },
      "cell_type": "code",
      "source": "import signal\n\nclass DelayedKeyboardInterrupt(object):\n    def __enter__(self):\n        self.signal_received = False\n        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n\n    def handler(self, sig, frame):\n        self.signal_received = (sig, frame)\n        print('SIGINT received. Delaying KeyboardInterrupt.')\n\n    def __exit__(self, type, value, traceback):\n        signal.signal(signal.SIGINT, self.old_handler)\n        if self.signal_received:\n            self.old_handler(*self.signal_received)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c22cc0bc3213b4dabb69ff57cb285dafddbeefb7"
      },
      "cell_type": "code",
      "source": "# Exponential average of loss\n# Initialize to a error of a random model\nsmooth_loss = -np.log(1.0 / X_size) * T_steps\n\niteration, pointer = 0, 0\n\n# For the graph\nplot_iter = np.zeros((0))\nplot_loss = np.zeros((0))",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c0c833a7db3b879a797ba390dc1455e984d0c52f"
      },
      "cell_type": "markdown",
      "source": "Training loop"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "899ddea9a546975c49e97c32b68d150b777256c5"
      },
      "cell_type": "code",
      "source": "while True:\n    try:\n        with DelayedKeyboardInterrupt():\n            # Reset\n            if pointer + T_steps >= len(data) or iteration == 0:\n                g_h_prev = np.zeros((H_size, 1))\n                g_C_prev = np.zeros((H_size, 1))\n                pointer = 0\n\n\n            inputs = ([char_to_idx[ch] \n                       for ch in data[pointer: pointer + T_steps]])\n            targets = ([char_to_idx[ch] \n                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n\n            loss, g_h_prev, g_C_prev = \\\n                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n\n            # Print every hundred steps\n            if iteration % 100 == 0:\n                update_status(inputs, g_h_prev, g_C_prev)\n\n            update_paramters()\n\n            plot_iter = np.append(plot_iter, [iteration])\n            plot_loss = np.append(plot_loss, [loss])\n\n            pointer += T_steps\n            iteration += 1\n    except KeyboardInterrupt:\n        update_status(inputs, g_h_prev, g_C_prev)\n        break",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD1CAYAAACm0cXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XeYVNX5wPHvbGEXlt6XoisIh6oIIoiUVZEeTQRjIhKJRmxRsQZLQLAR/RlUNCqKKGgUxRhABKRIldBUkHakLYKwLL2zjfn9MYU7M3d27rTdvbvv53l8nL1z586Zy8x7z31PczidToQQQthTQkkXQAghROQkiAshhI1JEBdCCBuTIC6EEDYmQVwIIWwsqTjfTCmVAnQE9gGFxfneQghhU4lAOrBaa53r/6SlIK6UagNMB8Zprd9QSnUHXgDygVPAEK31EaXUY8BNgBMYrbX+2u9QHYGlEX8UIYQov7oBy/w3hgziSqk0YDywwLD5n8BgrbVWSj0J3KWUmgr8AbgSqAYsVUrN1Voba9z7AD7++GPq168f8ScRQojyIjs7m8GDB4M7fvqzUhPPBfoBfzNsOwjUcj+uAWjgamC21joPOKCU2gW0An4yvK4QoH79+jRq1CiMjyGEEOWeaQo6ZMOm1rpAa33Gb/NDwH+VUhpXFf8DoD5wwLBPDq48jhBCiDiJtHfKeOB3WmuFK0dzr8k+johLJYQQwpJIg/glWuvl7sfzgMuBvbhq4x4N3duEEELESaRBPFsp1cr9uCOwFVgI9FdKVVBKNcAVxDfFoIxCCCGCsNI7pQPwCpAB5CulBgF3A+8qpfKBw8DtWuujSql3gSW4uhjeo7U+F7eSCyGECB3EtdZrgUyTp64y2Xc8rny5EEKIYmCbYffNnvqasbO3lHQxhBCiVLFNEM8vdPL24u0lXQwhhChVbBPEhRBCBJIgLoQQNiZBXAghbEyCuBBC2JgEcSGEsDEJ4kIIYWMSxIUQwsYkiAshhI1JEBdCCBuTIC6EEDYmQVwIIWxMgrgQQtiYBHEhhLAxK6vdo5RqA0wHxmmt31BKJQMfAhcDJ4BBWusjSqnBwHDgHDBBaz0xTuUWQgiBhZq4UioN10IPCwyb7wQOaK2vAKYC3dz7jQR64lpE4iGlVM2Yl1gIIYSXlXRKLtAP30WPfwN8DKC1nqC1ngF0AlZrrY9prc8AyzFZ/UcIIUTsWFmerQAoUEoZN2cAfZVSLwHZwL24Vro/YNgnB0iPWUmFEEIEiLRh0wForXUmsAF4Isg+Qggh4ijSIL4fWOx+PBdojSvdUt+wT0N8UzBCCCFiLNIgPhvo437cAdDASqCjUqq6Uqoyrnz40uiLKIQQIpiQOXGlVAfgFVx58Hyl1CDgFuA1pdQdwEngNq31GaXUCFw1cycwWmt9LG4lF0IIYalhcy2uLoP+bjLZdxowLfpiCSGEsEJGbAohhI1JEBdCCBuTIC6EEDYmQVwIIWxMgrgQQtiYBHEhhLAxCeJCCGFjEsSFEMLGJIgLIYSNSRAXQggbkyAuhBA2JkFcCCFsTIK4EELYmARxIYSwMQniQghhYxLEhRDCxkIuCgGglGoDTAfGaa3fMGzvDczRWjvcfw8GhgPngAla64mxL7IQQgiPkDVxpVQaMB5Y4Lc9Fdcq9/sM+40EeuJaCeghpVTNGJdXCCGEgZV0Si7Qj8CV658E3gTy3H93AlZrrY9prc8Ay3EtliyEECJOQgZxrXWBOyh7KaWaA5dqrT83bK4PHDD8nQOkx6SUQgghTFnKiZsYBzwQYh9HhMcWQghhUdi9U5RSDYEWwMdKqf8B6UqpxbjSLfUNuzYkMAUjhBAihsKuiWutfwWaev5WSmVprXsopSoC7ymlqgMFuPLhw2NWUiGEEAFCBnGlVAfgFSADyFdKDQJu1FofNu6ntT6jlBoBzAWcwGit9bHYF1kIIYRHyCCutV6Lq8tgsOczDI+nAdNiUTAhhBChyYhNIYSwMQniQghhYxLEhRDCxiSICyGEjUkQF0IIG5MgLoQQNiZBXAghbEyCuBBC2JgEcSGEsDEJ4kIIYWMSxIUQwsYkiAshhI1JEBdCCBuTIC6EEDYmQVwIIWzM0so+Sqk2wHRgnNb6DaVUY2ASkAzkA7dqrbOVUoNxreZzDpigtZ4Yp3ILIYTAQk1cKZUGjAcWGDY/hytI9wC+BB527zcS6IlrEYmHlFI1Y15iIYQQXlbSKblAP3wXPb4X+ML9+ABQC+gErNZaH9NanwGW41pnUwghRJxYWZ6tAChQShm3nQJQSiUC9wFjcK10f8Dw0hwgPZaFFUII4Svihk13AJ8CLNRaLzDZxRFxqYQQQlgSTe+UScBWrfVo9997cdXGPRrim4IRQggRY5Z6p/hz90LJ01qPMmxeCbynlKoOFODKhw+PvohCCCGCCRnElVIdgFeADCBfKTUIqAucVUotcu+2SWt9r1JqBDAXcAKjtdbH4lJqIYQQgLWGzbW4ugyGpLWeBkyLskxCCCEskhGbQghhYxLEhRDCxiSICyGEjUkQF0IIG5MgLoQQNiZBXAghbEyCuBBC2JitgviAS2Q+LSGEMLJNEE+vlkqlCoklXQwhhChVbBPEAZzOki6BEEKULrYJ4jKvrRBCBLJNEBdCCBFIgrgQQtiYBHEhhLAxWwVxadcUQghfllb2UUq1AaYD47TWbyilGuNaXzMR2AcM0Vrnulf8GQ6cAyZorSfGqqAOhzRtCiGEv5A1caVUGjAeMC6GPAZ4U2vdDdgG3O7ebyTQE9ciEg8ppWrGvMRCCCG8rKRTcoF++C56nAnMcD+eiStwdwJWa62Paa3PAMtxrbMphBAiTqwsz1YAFCiljJvTtNa57sc5QDqule4PGPbxbBdCCBEnsWjYDJasliS2EELEWaRB/KRSqqL7cUNcqZa9uGrj+G2PGRl2L4QQviIN4vOBge7HA4E5wEqgo1KqulKqMq58+NLoiyiEECKYkDlxpVQH4BUgA8hXSg0CBgMfKKXuAnYBH2qt85VSI4C5uLp0j9ZaH4tbyYUQQlhq2FyLqzeKv+tM9p0GTIu+WEIIIayw1YhNIYQQvmwVxJ0y8F4IIXzYJoiX9lH3I75YT8aIWSVdDCFEOWObIF7afbp6d0kXQQhRDkkQF0IIG5MgLoQQNmZpKtrSYM+RM+QXHizpYgghRKliq5r4/uO5oXcSQohyxFZBXAghhC8J4kIIYWMSxIUQwsYkiAshhI1JEI+x03kFLNsqvWiEEMXDdkE85/jZki5CkUZ88RO3TlxJ1sFTJV0UIUQ5YLsg/vt3VpR0EYq0LeckACdzC0q4JEKI8sB2QTzr0GlOnM0v6WIEJfMsCiGKU0QjNt3Lr00GagApwGggG3gLVxxbr7W+J1aF9DdnQzY3Xd7Y8v5Op5OlWw/S9eLaJCQUz3SIpXnWxcOn8vhk1S/cm9kUR2kuqBAipEhr4kMBrbW+GhgEvAa8Cjyotb4KqKaU6hubIgZa/POBsPafvSGbP72/ig9XZMWlPEZOG6zm/Lcv1vPyXM3KnYfj/l67Dp1i3qb9cX8fIcqrSIP4QaCW+3EN4DBwkdZ6tXvbTKBnlGUL6qv1+9h16BRn8got7b/36BkAdh8+E68iBXBQemu4p9z5+sJz8b/gXPPKYu6cvCbu7xON3IJCDp6UKR2EPUUUxLXWnwIXKKW2AUuAR4Ejhl1ygPToixdcj5cXMWzK+eDgdDoZO3sLW/ef8G4bN+9n1u46YvZyARTHTUNxXCiide9H33P5c/NLuhhCRCSiIK6UuhX4RWt9MXAN8JHfLsVSDV1q6I998GQeby/ezi3vrfRue23BVga+9V1xFEXY2IItOcX2Xh+v3MVbi7YX2/uJsi/SdMpVwFwArfU6oCJQ2/B8Q2BvdEWLTGnJSUt7oTDz1Jcb+MecLSVdDFGGRBrEtwGdAJRSFwIngM1Kqa7u528E5kRfPPspJdeQIhXHBWbyiiz+b66O/xsJUc5FuijEO8D7SqnF7mPcjauL4TtKqQRgpda6XCcZ7VATd8axV/vI6RvjdmxRdszftJ96VVNp26haSRfFtiIK4lrrk8DvTZ7qFl1xSta8Tfu5c/Ia1o3qRbWKyREdI56BUYiy5i/unktZY/uXcEnsy3YjNiNx8GSepf3+tWgbAJv2Ho/6Pa12MVyweT8jp2+I+v1EeE7mFjBy+gbL3VTLk38t2sbj09aVdDGERWUuiJvlpN9e7OoNcOR00cHc89o/vvu/mL5/Ue74cA2TV+yK+P2sOnY6n4wRs1iweb/lC0xB4Tlvn/Ky5l/fbmPyil1MXpFV0kUpFaau/oWMEbM4cTafl+ZoPluzp6SLJCyyfRDPGDGLViPn+OSgx87eYjrApDhHDgbLiZ/NL+SGN5bxwy/F239du/vPey5oVtz/yQ+0HjU3XkUqUYXuq60NurEXi3eW7ABkHVs7sn0QBzidV8hPvx7z/v324u2mATvUzIKx+D2HOsbmfcdZt+cYz8zcFIN3i6/ZG7JLugilytHTeeScKN1TIYvYmL9pP6NskuYsE0Ec4M+TVofeqRRzTdJ1IO793I2Ht0N3yHgLpyG63Zh5XPH8goje5w8TVvDMjFLcY0e+Cz7+MnkNH7rTnCfO5pfqtpMyE8Q9Dp2y1ohpyhDV8grOsXHv+dr96bwCzuZH/g/pdDo5bCybXwT9fM0ehkxcxX++/zXi9/DYe/QMGSNmsW73Ue82Y3rHDt0f462457b5347DfPBdVrG+ZyTkuxGo7TPf0O2lb0u6GEGVuSAeqZ0HT7Fuz/mgPearjfR/fRnLtx3kwIlcWo2cS6cXfGthS7ceYEu2b0+WYDXpySt20f7Zeew4YL7iz+4jpwH49Wh4k3TN3ZjN9B99A79nlsdPVv0S1rFE+SUV8aKV5gnSIh3sY2t5BedITnTQ6YUF9GubzjPXt+b6N5b57PPDL65a7GDDXCzHzvguRjFk4irAt4/rdneQ9q/QeAJr1qHwl207nVfA3I3Z/O6yRgHP3TVlLQA3tGtY5DHMri2nS/EtoihenspHOBXxn/efoHJKEg2qVwz7/TJGzOKR65qH/ToRqNzVxJ1OJ82fns2oGRvJOZHrvcU9cTa2Xek8MTP72FnW7znq/XFEkod+ZsZGHpq6jtVZrvm/sw6eiiq143H3R2ujPka08grO8d7SHeQXnivpoggIa5GQXuOW0GXswojf65V5P0f8WnFeuQviM9a55uUy9s2OJO3wT8MX8MCJwFut95ft5MTZfDq/uIDr31junSkvkhGd+465ekScyi3gVG4Bmf+3iMemrQ/rGKU11/n+8p08N2szH/0v/n3lQ/EM9iqPzL6VeQVyYbWDchfEn/zPTwHbnjDZFqqm+/qCrd7H/V5fGvD8ml1HGGXSGyFUTfzE2Xz2HzfvxpbgcHjLtWxr6NWNoul9Yhyx99nq3d7HOcfPkltQyJFTeeQWWL8bWL/nqOl2z3qppWFQ0UtzZMIu47VeRhL7Kq13i7YJ4r+5tEFMjmM1rm0P0gD59U/7GDJxpc82s5r4tpyTpj1NjO9/1DCC1BNw3126k04vLOC7ba650icu2+mdN91Ymz6ZW+CzAIZRUZVuq5/fOGLv8S/O1/qveGEB9338PZc9Oy+sbp2b9xU9lYHn8y/SOcxav8/ycQG25Zwosmum0+nkUJCGqUgudKW5hppbUMicDeGdv2D+t+NQTI5TVrwTxkC54mSbIJ5eLTUmx4m2b/S9H3/vsxhFxOUAOr8YvM+xZ3GLZ786Pyho2to9PPq5q4acX+jkunFLvM8VVUvYuPcYWQddF6W1u454G23NPD9rE0tD1PLnb3alhr7bbv1HHqwR1b+r39BJq7nv399bPu7qrMP0/OcSprjTMRkjZnHHB74Xlyn/20WH5+b7XPTCTS9d/X+LvI/v/TiwfIt/PsDpPOt3EwVB/r2OnMrzufMJ19jZW7j7o+9ZEca/DZj/LqTHiq/Dp/KLfP6lOVv4cXfw31a82CaID+2SEZPjlPQsg/MNI0nP5p//IVsJKtN/3Mu32jzA/u2L4Dny/q8v88mhFzVy9d2lO729bsKxSOfwrc5hkc4xrRUXnnNyJq+Qz9fsNn0+0n+Vne6L0/o9x7xLwfmv1LPk54M++8L5oLXXYpdO42vnb/YdDZx18BS3vb+Kx8Nop1gTZNnABz79gce/WM+2nJOWj2W054jr8/j3pArF87uIpO3Ec9do+b2KcZTZ4VN5LA+zfJH616Lt/PbN5THpdBAO2wTxSLoxmTEGzpKw1f3jjPXak//5/lefwUkAU9fs5j/fx2Yio+Nn84tc8HjopNX8edJqhk5azZc/mKSRnPDC15t5bNp6nzuZWF1UnU4sf9asg6e8teaPV0bfl36D+7wXdYdjpvCcM+CC6knNRZuyCTcYe+Kq8c7IaqzdcdA39XjibD4Pf/Zj2BcSMydzC9h9+HTEr7/1vZUMfm9l0DufcFj9rsbic4cj4n7iSqnBwONAATASWA9MARKBfcAQrXXp7SFfwvxj+PYDkdW8jPq/vixgXuYpMer18dnq3ZYnEAtWu/UEKGMj5pvfuvKMkVbOjrt/MHM3Zvv82M+dc5KQYB7JMg2pkVCyj50NGNDl742Frl4txoFaGSNm0TGjBte0qMfdPZoEvMbphKf/u4FPVv3Ctuf7kpToqk95GrUjvbh5zuO8Tfs5k1fI+8t38umwzlSqYO2n/tbionvo/Hr0DA39KlRZfkH8DxP+x8a9x6lXNZW/9WkBwMx1e7myaS1qV04J69/65ndWsHHv8YjnG//ZnUIry6mhSBdKrgWMAroCA4AbgDHAm1rrbriWb7s9VoUsi/L8enaYjeT05L/D8cyMjT4XiFC1w3PnnLz57TaOnY5d7SHYj3TjPleN9Z6Pv+eBT34IeN5/5KnH5c/N58WvN5s+99ws1/aTuQWscvejB9cMjN9q37TKsClrWRskjRFM71eXMNSkAXfv0TOMnL6BgsJzQT/v6qwj/GPOlqAX6C/Wuu4cCp1Odh8+zZqswxyJ8t/Bczc2be0ehk/9kfV7jvH0l9Z7mXyyKng+/qv1e7lq7ELeW7rD5w7ivWU7/crguuh5aqQHT+Zy/yc/8JcPg9/J+ct8+Vt6j1viPVakCtw/hgMncmNSG7fCv+rw3baD/P6dFXFLI0WaTukJzNdan9Ba79NaDwMygRnu52e69xFB+Pd+2ZId2NNk2trwUyEffJfFrsPWR4Uu23aQl+dqnvpvYDdLo3AGgQTLFO0+fL6mOmPdXpo/Ndvn+Qc//dH0dQdP5nqnSp2zYR9rsg7T+YUFRfYtn/XTPtPeMyu2W8+PHjyZG/TW+NHP1zF5xS5WZR32TvMbjJXMWbeXvmXQ2yu8f0f6e/eMKTD6j0l6y+hsfqGl9/vRXSF4btZm2liYothzzIJCp7tsrn9/Kx8t69DpkOc1HF3GLuThz9ahTX5nRckYMSvoc4t/PkDmy9+SW1Do24XW76dyy3srWbXzcNQXpGAiDeIZQCWl1Ayl1FKl1LVAmiF9kgOkx6KAInyfhzGh/22TXI2YofppBwvhL3y9mSN+k44Vngus8ZilB/IMNaNx862N3rv7o+8Z9PYKso+f5en/WqthRjrQKdskIHqcc0eoovbxCJZe8nz+42eCn/uLnpjF0Em+Dc2frPrFdErctbsOB2zzOHE231sTfeUbTcaIWRw4kcu+Y2do8fc5pnP2hJPSGTl9AxkjZvnlr31ff+BELj/8coSFfg3PHvd9/L2li0OkZqzbS+9Xl4Te0aJR0zeQdeg0V7+8iOvfWH7+CcPHNp6PWLeDeUQaxB1ALVyr2g8FJuH7Oy+l4wPLh8NhzOToqS2F+noFC4QTluxgzFe+c6O/vjB2Ix9j0dJvrGWG6hZ5Nr+QSct3ci7ED87TAHjGQvnM0jHGH3TH583XFN+87zhOJywy9Ej69egZnvjPTwybHDhlQlENam2f+YbhU113OuPd/z4dn58fVnrpdBGf1TMC2thu4n8tP+eE3/3ru6AN5LN+2hdyzv9weEZn+/OvdFgV7G5lr9+F/JmZ5wf5FdVrLFYiDeL7ge+01gVa6+3ACeCEUsrT4tEQMD+DolQKdTtd1I/9ewurFBlTKVbo7BPkFZyjxd/nhPU6f3P8FrYIFcTHzfuZ0TM3MXN90V9fz0XtvaW++eCMEbMCGvrM3Oo3YMzfgPHL6Pta4EjgfHevFbOlBkP9G361fl/AaOBdh6z3/Pi3hZ48xiIcP5vPqp2Hw26kfdedOjM6mVvAmJmbTC/qR4Msu7hqp/m/9WXPzgurPOH6+qdsDp/KY+/RM77z98fp/SIN4t8A1yilEtyNnJWB+cBA9/MDgeh+faJYnQsRAb4qYhSllUAQbi+Z3q8uYfTM6BdRuPujtQH9uoviqc2eziskt4hufp4gvtMkYP+wO/ZL71m5u7KS1/afTvnluUVPNeB0OiNuEJy9IZvfv7PC0myZxrmInjdpxG4zai7vL9/JP+Zs8Qnka7IO027MPNNRqkWdjxnr9rL9wEnvHdfinw9w/Gw+327JQT09O/gLDbKK+N63f3ZeVJODhSOiLoZa61+VUtMAz4rC9wOrgclKqbuAXcCHsSmiKK/8+3CHOxw/WgPf+i7oc0UtKhGPTgjtn53HNw91JznRt971zcZsLrugBnWqpJBjMv1DNJxOGDd/K68v2MqmMb0tvibww4dKTYHvXERFmbQ8i1+PnGHCny4H8K4BsHLnYfq08W2GK+ptjb2jVj/Vk9veD2+Am2fOn9Ig4n7iWut3gHf8Nl8XXXFESXE6gw8FLy3CGY4fqU/dQ95DBeKiGkvjNSBxx4FTfOtuFNx16DRzN2Zz15S1NKtbmXkP9+DJL4vuYRSuPUfOeIPryRhP1RyNbzbtJ2PELHa80M+7bdLyLIZ1b0J6tfAHBb40Z4ul/T74LotW6VXp1bqe5QW2jamkeA0Css2ITRFfy7Yd5Of90Q84EvBIBP37rVj8cw5T15zvx+1ZEGRrzknGW6zJliVOfGv+D09dR//Xl7It54Rhj9A+D6Mr7+NfrKfdmHkR9dyI1ehpfxLEhZfZlLrlVaxrtbHgmXjMTLwXWLB6c+E/BB/wmagtnlbsOMTGvccZN991QStq4FK0Eiz2Wy2OaWJsFcRrplUo6SIIAYQ3+ClWzKY8Li53fGht2mErPVhixel0mg7eOXo6j+Nxzll71sQNR7wCuq2C+KwHupZ0EYQAYE8UkzLZ0YZf4zPaMBrzNu03TYUs33aIy58z73sfKwPGLwu9E753MKWti2GJiKTRQoh4MEsbiOJ1j8m87h6lZeGOUIuhxIKtgrgQQtiJcQH20jYBVompkGS7IgshhKRTPK5sUqukiyCEEGH7McxFQ6yyXRD/1+D2JV0EIYQIm9lMkbFguyCelhLxIFMhhChzbBfEAYKsuiWEEOWOLYP4iieuLekiCCFEqWDLIF6vampJF0EIIUoFWwZxIYQQLrYN4n3b1C/pIgghRImLqquHezm2DcCzwAJgCpAI7AOGGBZOjrnxf7yMi5+ytgKHEEKUVdHWxJ8GPEtsjwHe1Fp3A7YBt0d57CIlJdr2JkIIIWIm4kiolGoBtAJmuTdlAjPcj2cCPaMqmQVN6qTF+y2EEKJUi6Y6+wrwsOHvNEP6JAdID3xJbL3xRxm9KYQo3yIK4kqpPwErtNY7g+xSLMNxqqTK6E0hRPkWaRTsDzRRSg0AGgG5wEmlVEWt9RmgIbA3RmUUQggRREQ1ca31zVrrjlrrzsB7uHqnzAcGuncZCMyJTRGDq18tlcsvrOH9e0TfFvF+SyGEKFVi2cVjFHCbUmopUBP4MIbHNpWcmMC0e7p4/66amhzvtxRCiFIl6qSy1voZw5/XRXu8SDzWW/HyXE3dKinebX1a12fOxuySKI4QQhSbMtEyeHePprRMr8LVqm5JF0UIIYpVmQjiiQkOrmlRr6SLIYQQxa5MBHGjZ3/bhia10ziZW8Ccjdk0q1uZrTknS7pYQggRF2UuiA/pfKH3cdbY/hSec9L0ya8BmPTnjvx50uqSKpoQQsRcmZ+AJNGwDJDkzIUQZU2ZD+JWjLv5UlqlVy3pYgghRNjKXDrFTIv6VejbJvhULr+7rBH92qazdtcRbnl3ZTGWTAgholMuauJzhnfnwZ7NitwnJSmRLk1r8/atMqmWEMI+ykUQN0pOdOXIM2pVMn2+TxE1diGEKG3KXRB/4XdtqVMlhQWPZJZ0USKy8JEeNKntmkd94m2XU69qSohXCCHKsnIXxG+6vDGrn+pJYoKDibddzpzh3QL2mTCkA4/1Vt6/p9xxhemxLqp9flGKmy9vHFY5jMcPR5M6lb2PM2qn8e2jmax9uqe3jLUrVwh4zWt/aBfRewkhSr9y0bAZzLUtzUd59mpdn+SkHO/fqcmJNKmTxo4Dp3z269G8Dn8f0JIfdx/j4euaM3XN7riW1+PNwe15d8kOMmqlkZjgoFKFJJrXqwJAgsNBjUrJHDmdD8D1lzbghnYNcTph+NQfi6V8QojiU+5q4pFo17g6Cx/JZLhJ4+g1Lerx8HXNTV9XVA04wRG4bkaVFGvX1JbpVfnnze18+sCfczq9x3UYju10/z8lKfCf+t9/6RTyvT6760pLZRJClAwJ4iH0aF6HZPeizMN7Nmfni/24s9tFpvt+//fr2Di6N+tG9WLt0z25oV3DoMe9poXvwKPP776ShY9m+mz7dFhn7+PkRAfp1VKDHq9iciIAHTJq+Cyr5Anuzdw1daMuF9c2Pda0u6+kbcNqAJhca0qtrLH9A7ZVr1R+pycO1ngvyhYJ4mFyOBzUr1bR9LmaaRVIS0miWsVkalUuusHx4rqVff5u27Aadar4vqZzk1psebYPbw1uz9bn+7HiiWuDHq96pQp8/UA3XrnpUv55c+AdwMV1Kwe9Y/B3eUZNJvypAw9c24yWJoOg0iokWjpOcXhrcHs6XVTT+/cTfVsw6jeteO63bQDo26Z+wGtqh/i3KUqmqhPxa4vboseuLukiiGIgQbyUSE02D4ypyYn0bWut22OrBlVJTU6kR/M6jP/jZa6NzvPPVzaka4I11n5tTqAiAAAO8ElEQVR5r2uRjfRqFYMG/e7N63Bti8ApDIq6U4ilx/ucbxTu2zadyXdcwbpRvQC4q0dT/nzVRTjddyCJCQ7vZ/KokBj57cUN7RpE/NriUK1i4J1HavL5n3mjGoEVELN/y7LISvoQgv82SquIg7hS6iWl1Aql1Gql1I1KqcZKqUVKqaVKqc+UUrbu+9auUXUSHHBXjyYBz0WbYWhQDMHOLA0ysH0j7+NuzcxrlJddUMN0u7+JQzsGbHtp0CWsfNJ1t9C8nu+dRpM6aXxyZ+eA13iE01VyaJcMHrmuOWNuaA24Bmr5B6/OTWoB0K9tesBncmLuyX6hl/dzmPzr/31AK0uvLQ6pyQksH3ENS9y18Jl/7cqSx6+mWzNX6mx4z8ALc90y3E3V8x2B83e/TQy9ysqCSFe7vxpoo7W+EugDvAqMAd7UWncDtgG3x6yUJaBGWgV2vNifLk3N88aRWvxYJrMf7E5igoMn+7Vg5l+7snlMn5i+B7iCWFKCg9u7ns/fVwuRH+7XNjD1YNYg6gwSBR04qFc1lS/uuZJPh51vEB19fWsWPpJJRm3zHG2dKimMHNDa9Dl/WWP7U6lCEvdf24w/XZkRdL9m9aqQNdb8369ulRSfC5pHUROkJSUEv3RXSUnizm6BF/uS8GS/ljSsXpEL3Pnwto2qUbdKqvc8tGtcPeDf1BFFw8dVF9cK2Lb26Z5hd7mNl1ppKcx/uAezHuhK3aqpTB3WmZn3d/U+v6QYU04f3h6fGn6kNfElwE3ux0eBNCATmOHeNhPoGVXJSrFa7r7YVmowTer4XvUvrJXmDabDujelbaNqVDTkmM2CZiRqV05h2wv96HChtZp16wZV+dfgDgHbkxMTvLU4j7aNqnkft0yvGvBD7nBhTWoYLhiDOjTyHstMcoKDCn6fO9a9YjznoXvzOrx3W0fTBk//WJY1tj8PuWuuU+/qjH6uj/n5DBIDZz8YOAbBo3FN83YVj+svjSxt07C6+XHv6t6E5SOu4eK6lQPuRIKF8K5BGr4B/jGwLW/f2p4W9QPbTJISEujVOrxFWszaXmLB4XDVwFs3cH1nOzWpRVpKEv8Y2JZGNc5f7IyCVVKKMv2+q1j8WKbpc9e2qMv/3XQp3ZvFtkLoEelq94Vaa0+n6TuAr4E0rXWue1sOUGbHr19/aQNe+0M7hlmofU2/7yqWj7jG8rHXPN2TR65rHpDHjZdP7uzMSwMv4StD7cTflDs6eedp/+MVF3BPj6YA/PxcX2b+9aoij98yvSpp7lx8sAbFbs3qcEHN8z+mhY/04IqLavLD332XbH3dk+ePgCe4DWzfkDpVUni0l2LkgFbe57s0DaxRAvz1mov5731X0eHCmqQkJdK4ZiWfnDy4gqBZbbZlelWe6tcS8E0XPffbNnxxdxdG/aZVwGs8jHnsqqnBu55uGN2bL+4J/V1JSHB4z4HTQpRKSnBQxe99Vzxx/ns8sH0j+rRJN52TyInTNJ0XLI2YUasSk+NUS23dwPzicHPHC1j2N+u/y1AN2pc2rs6FtdLof0lg2Js4tCODOjSK6o6nKFFV+5RSN+AK4n/1e8pGHdPC53A4uKFdQ5KC1CyNqqQm07B6RV77QzvG3Xyppf3vv7aZ5dx0uDaM7s1Pz/Ty/n1l01r8vmNjy1+wlulVSHCnFiokJQQ9Bw6Hg6nDOodsTFry2NU8+9s2qPpVuLF9Q5rUTvOOSq2R5jv6NNLaqZEnflWskOiTanprcAcqpwTWzhMTHLRrXN1n272ZF1tuxL2zexMmDOnA0/3PB+zfXNqAulVTudEkpQOugJBpSO38sdMFQY9fOSXJ5+7ASpfKc34xXNWvwr2ZTX22bXm2j0+NNL1aKunVKgYE56qpyXx+d+Bdk1nbwXdPXMuN7QO73S54JJM6VVJ41aRXVbj82yYurBV+/tvsEjfqN4Hpvr90DexqXNSFOV6iadjsDTwF9NVaHwNOKqU893INgb0xKF+ZcUO7hvzuMvMfbXGqnJJEldTY9p0OVrHr1KRWQCBe8tjVLHo0kwlDOjBneDcuqFXJm0r55+/bBfSVj5VQ16hqlZKpH2GDs6fXT5uGVU0HePVqXZ9eresx4JJ0Zj/YzdsI698Y27NlXRY80oP/3tuFfoYeSSP6hG40reC+mF5cN3A8gL8JQ86nzT5y32U93qcFkwyN1UmJCT4Xj8buOyWz09gxo2bgRveOPZr71mA9f//W0MvHM2itr1+bjGdai5ppFXxSU31aB7bdeCiT9E64zLrQXuTXGNq8XmWeHhAYsOtWSTUdrxBPkTZsVgNeBgZorQ+7N88HBrofDwTmRF88UVp4fnz+tVIjK5X5C2pVIqN2Gr1a1zfNp/ob2iUDCJ7rDZczaN8UXwNMbovN3JvZlD7uvuhf3d8t6ACvlKRE3rilfUDu95M7O3s/29AuF9G0TuWAuyKHw2FawzOmm34cdZ3PHVZRuhpys5c0rhb0LsyYw63uvuBYqdk6nXCx+27K/zze0K4hq568lnE3t+Ozu65k/sM9vM+lJCWy/YV+3r9nP9iNLk1rMe+h7j7nbVwRNfYuTWvxxyuC37mYMXYbffXmdlyeUZNNY3oH7Ncx4/wdT7jvEU+R1sRvBmoDn7m7FS4CngduU0otBWoCH8amiKI06NmqHlue7cMljYIH8Xh45vrWZI3tH1a7gpmiri/Gniorn7yWJY9dHfLW/ol+LUlNTuDBns2iynVe2bQWF7ob1/wPs2F0bzaMdgWTP191EZcYGpRfvbmdz11OpQrW77BSkhK9UzwY76JSkgN7rfytTwsGXJLOS4MuAWDqsM68fWuHgDSaMY2R4HDQuGYltj3fl5tMeqnUrZqKw+HgiotqBgx6M04lkZqcyL/v7BwwcK5ihURa1De/40hOTODFG9sG++imjA24v73MdRGuVCEpIGWmDO95SxEpLoBXbgqdOo2ViCbA0lpPACaYPHWdyTZRRgQbkGQHldxBy7+HzNbn+5JoiJ71qlpLqVx/aYOY5OgB+rSpz3fbD/k07oLv4CyA9hfUYP2eY3x5b5fo20xMrjuelEz7C85fqO/xy5XXrZrqvfMwGta9Kf0vacAinePtfWWlzSgcnZvUpEdzV1vBnOHdyRgxy/vc0/1b0umi843TE4Z0sPxvGaxnTL+26UxcttN7DkZf34a1u44yaWhHUpJcv4Wlj1/NoVN5Aa8d2KERj3y+ztoHi1K5nsVQxMb1lzbgu+2HAvKGpckTfVtQr0pqwDJ9wbo9FqchnS9kYPtG3l48wTzVvyU3d2wck+543hgeQXe6YBpWr8jgThfG7oB+jGMPjAa2b8Rf/HqK9Soib+6vTcNqptuT3CN7q7rvcBITHAHdRhvXrORtL/C35LGr2X/irOVyREqCuIjazR0bM6hDo5jXvGKpSmpyyCX6SorD4QgZwMF1wYlVf+pPh13Jf77fQ9WK59/3sgtqMLjTBdzVvWkRr4y/WQ90ZdnWg5b3f+X30acu+l+Szuyf9kV9HKMLalUy7YceaxLERdQcDoe31iLggWsu5vWF20q6GEVq1aAqrRr4NpYmJjh4/nfh5ZPjoXWDat7BOUWZcscVLNySE3I/K968xb5r60oQFyLGHu6leLhXZCs3lUVNaqex4+Cp0DuGqVuzOkHnAIqFv3Rtwqa9x/lDx9IxhUAwEsSFEHE1Z3h377z2dlKnSgpT7rA282FJkiAuhIgr/3lxRGzJ2RVCCBuTIC6EEDYmQVwIIWxMgrgQQtiYBHEhhLAxCeJCCGFjxd3FMBEgOzu7mN9WCCHsyRAvTWegK+4gng4wePDgYn5bIYSwvXRgu//G4g7iq4FuwD6gsJjfWwgh7CgRVwBfbfakw8qiqUIIIUonadgUQggbkyAuhBA2ZosJsJRS44DOuNYheVBrbZobKouUUpnA58BG96afgJeAKbhyZfuAIVrrXKXUYGA4cA6YoLWeqJRKBj4ALsTVDvFnrfWOYv0QcaCUagNMB8Zprd9QSjUmynOilLoUeAvX92y91vqeYv9gMWBybj4AOgCH3Lu8rLWeVU7PzUu42uWSgBdx5Zlt/b0p9TVxpVQPoJnW+krgDuD1Ei5SSVistc50/3c/MAZ4U2vdDdgG3K6USgNGAj2BTOAhpVRN4BbgqNa6K67FrF8skU8QQ+7POh5YYNgci3PyKq5KwlVANaVU3+L4PLEU5NwAPGH4Ds0qp+fmaqCNO5b0wfWZbP+9KfVBHLgW+C+A1nozUEMpFZs1quwrE5jhfjwT15etE7Baa31Ma30GWA5chev8fened757m93lAv2AvYZtmURxTpRSFYCLDHd5nmPYjdm5MVMez80S4Cb346NAGmXge2OHIF4fOGD4+4B7W3nSSik1Qym1TCl1HZCmtc51P5eDq/uR/3kK2K61Pgc43V8829JaF7h/XEZRnRP3tiMm+9pKkHMD8Fel1EKl1KdKqdqUz3NTqLX2LDF0B/A1ZeB7Y4cg7q+8Lea4FRgN3ADcBkzEty0j2PkId3tZEotzUpbO0xRghNb6GuBH4BmTfcrNuVFK3YAriP/V7ylbfm/sEMT34lvzboCrAaJc0Fr/qrWeqrV2aq23A9m4UkoV3bs0xHWO/M9TwHZ3w4xDa51XbB+g+JyM5pzg+k7VMtnX9rTWC7TWP7r/nAG0pZyeG6VUb+ApoK/W+hhl4HtjhyD+DTAIQCnVHtirtT5RskUqPkqpwUqpR92P6wP1gEnAQPcuA4E5wEqgo1KqulKqMq4c3lJc58+TB/wN8G0xFr84zSeKc6K1zge2KKW6urff6D6G7SmlvlBKNXH/mQlsoByeG6VUNeBlYIDW+rB7s+2/N7YYsamUGgt0x9Xd5z6t9boSLlKxUUpVAf4NVAcq4Eqt/ABMBlKBXbi6OuUrpQYBj+HK1Y3XWn+slEoE3gOa4Wr0Gqq13l38nyR2lFIdgFeADCAf+BUYjKv7V8TnRCnVCngHV+Vmpdb64WL9YDEQ5NyMB0YAp4GTuM5NTjk8N8NwpZJ+Nmy+Ddfnte33xhZBXAghhDk7pFOEEEIEIUFcCCFsTIK4EELYmARxIYSwMQniQghhYxLEhRDCxiSICyGEjf0/ZuPmLR8+0z0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "----\n f tut the kitchen theaching and erma!\", ond age at rrabpely. Quick to sten still; he was to it wotle hust his fakis intiening to do stire he had beester open the voeling becamis pronce the clo-re, and \n----\niter 21273, loss 33.934170\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9915ae4f9b7989d48325ccbe9b8d13dc1941bdb4"
      },
      "cell_type": "markdown",
      "source": "### Gradient Check\n\nApproximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n\nTry this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56155ddf7415e5ebdc523e1340580b045b5e7f1b"
      },
      "cell_type": "code",
      "source": "from random import uniform",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6b8ffde2d50a3a83fe20ee31e87bf5f2de906112"
      },
      "cell_type": "markdown",
      "source": "Calculate numerical gradient"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b32348694dc6be666cee917e7aaf75eee5c8495"
      },
      "cell_type": "code",
      "source": "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n    old_val = param.v.flat[idx]\n    \n    # evaluate loss at [x + delta] and [x - delta]\n    param.v.flat[idx] = old_val + delta\n    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n                                             h_prev, C_prev)\n    param.v.flat[idx] = old_val - delta\n    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n                                             h_prev, C_prev)\n    \n    param.v.flat[idx] = old_val #reset\n\n    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n    # Clip numerical error because analytical gradient is clipped\n    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n    \n    return grad_numerical",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7db9bc97ab1c4c231bf3c8673e68fdf6936b6cbf"
      },
      "cell_type": "markdown",
      "source": "Check gradient of each paramter matrix/vector at `num_checks` individual values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f17ce2b30ee6aba0b3495ca563b1e0ca501cfb8"
      },
      "cell_type": "code",
      "source": "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n    global parameters\n    \n    # To calculate computed gradients\n    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n    \n    \n    for param in parameters.all():\n        #Make a copy because this will get modified\n        d_copy = np.copy(param.d)\n\n        # Test num_checks times\n        for i in range(num_checks):\n            # Pick a random index\n            rnd_idx = int(uniform(0, param.v.size))\n            \n            grad_numerical = calc_numerical_gradient(param,\n                                                     rnd_idx,\n                                                     delta,\n                                                     inputs,\n                                                     target,\n                                                     h_prev, C_prev)\n            grad_analytical = d_copy.flat[rnd_idx]\n\n            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n            \n            # If relative error is greater than 1e-06\n            if rel_error > 1e-06:\n                print('%s (%e, %e) => %e'\n                      % (param.name, grad_numerical, grad_analytical, rel_error))",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true,
        "_uuid": "381382cb73f189198b532d81d92f5928cb6eabff"
      },
      "cell_type": "code",
      "source": "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "W_f (-9.297452e-07, -9.290872e-07) => 3.538015e-04\nW_C (-4.693774e-05, -4.693753e-05) => 2.220947e-06\nW_o (2.192060e-05, 2.192144e-05) => 1.928718e-05\nW_v (4.484495e-04, 4.484484e-04) => 1.214722e-06\nW_v (-5.814371e-06, -5.814469e-06) => 8.390219e-06\nW_v (2.500347e-04, 2.500336e-04) => 2.049778e-06\nW_v (-2.300027e-06, -2.300382e-06) => 7.716535e-05\nW_v (1.180922e-06, 1.181071e-06) => 6.318756e-05\nb_i (4.915090e-02, 4.915078e-02) => 1.247316e-06\nb_i (1.922155e-02, 1.922151e-02) => 1.264812e-06\nb_C (-4.463041e-01, -4.463058e-01) => 1.955446e-06\nb_C (4.136349e-02, 4.136469e-02) => 1.450328e-05\nb_C (3.956657e-06, 3.956727e-06) => 8.805400e-06\nb_C (-4.463041e-01, -4.463058e-01) => 1.955446e-06\nb_o (-7.663016e-02, -7.662992e-02) => 1.598922e-06\nb_o (1.464580e-02, 1.464551e-02) => 9.999783e-06\nb_o (4.916689e-02, 4.916655e-02) => 3.452157e-06\nb_o (1.618785e-02, 1.618747e-02) => 1.168477e-05\nb_o (-2.921975e-02, -2.922002e-02) => 4.595204e-06\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b38ea3ce1eb0f3f4b64650fb875bc94ca885926d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}