{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport os\nprint(os.listdir(\"../input\"))",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['kafka.txt']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "with open('../input/kafka.txt','r') as f:\n    text = f.read()\nprint(type(text))",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'str'>\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "189e5a4bbb897d0fa117d0d1f601fdf609d47201"
      },
      "cell_type": "code",
      "source": "data = open('../input/kafka.txt','r').read()\ndata[:100]",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "'\\ufeffOne morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed i'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1eb83f8ce6f0dd75915a4def784514f175809c55"
      },
      "cell_type": "code",
      "source": "chars = sorted(list(set(data)))\ndata_size = len(data)\nvocab_size = len(chars)\nprint('data_size: ',data_size, '\\nvocab_size: ',vocab_size)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "data_size:  118561 \nvocab_size:  63\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "054fa369ac694726f95264a7de806ed3518ab0ab"
      },
      "cell_type": "code",
      "source": "# char_to_idx = {char,idx for idx,char in enumerate(chars)}\nchar_to_idx = { char:idx for idx,char in enumerate(chars)}\nidx_to_char = { idx:char for idx,char in enumerate(chars)}\nprint('char_to_idx: ',char_to_idx, '\\nidx_to_char: ',idx_to_char)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "char_to_idx:  {'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'L': 23, 'M': 24, 'N': 25, 'O': 26, 'P': 27, 'Q': 28, 'S': 29, 'T': 30, 'U': 31, 'V': 32, 'W': 33, 'Y': 34, 'a': 35, 'b': 36, 'c': 37, 'd': 38, 'e': 39, 'f': 40, 'g': 41, 'h': 42, 'i': 43, 'j': 44, 'k': 45, 'l': 46, 'm': 47, 'n': 48, 'o': 49, 'p': 50, 'q': 51, 'r': 52, 's': 53, 't': 54, 'u': 55, 'v': 56, 'w': 57, 'x': 58, 'y': 59, 'z': 60, 'ç': 61, '\\ufeff': 62} \nidx_to_char:  {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'L', 24: 'M', 25: 'N', 26: 'O', 27: 'P', 28: 'Q', 29: 'S', 30: 'T', 31: 'U', 32: 'V', 33: 'W', 34: 'Y', 35: 'a', 36: 'b', 37: 'c', 38: 'd', 39: 'e', 40: 'f', 41: 'g', 42: 'h', 43: 'i', 44: 'j', 45: 'k', 46: 'l', 47: 'm', 48: 'n', 49: 'o', 50: 'p', 51: 'q', 52: 'r', 53: 's', 54: 't', 55: 'u', 56: 'v', 57: 'w', 58: 'x', 59: 'y', 60: 'z', 61: 'ç', 62: '\\ufeff'}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "34c9e6d40e87b7bab52b48e3cecf75920cdbb66d"
      },
      "cell_type": "markdown",
      "source": "Hyperparameters"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cea8a1e61e269346815143df7ca94841695f2947"
      },
      "cell_type": "code",
      "source": "hidden_size = 100\nseq_length = 25\nlearning_rate = 1e-1\n\nWxh = np.random.randn(hidden_size,vocab_size) * 0.01 # weight matrix input x->hidden\nWhh = np.random.randn(hidden_size,hidden_size) * 0.01 # weight matrix hidden->hidden/memory\nWhy = np.random.randn(vocab_size,hidden_size) * 0.01 # weight matrix hidden->output y\nbh = np.zeros((hidden_size,1)) # bias of hidden\nby = np.zeros((vocab_size,1)) # bias of output y",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd45fb9ee6a1186150b7320277f94b6e6d810e60"
      },
      "cell_type": "code",
      "source": "def lossFunc(inputs, targets, hprev):\n    # inputs: (25, 63, 1), a sentence, contains 25 words, which word has a (vocab,1) shape vector\n    # outputs: (25, 63, 1), the label of input which has the same shape of input\n    # hprev: the previous state of hodden layer / memory\n    xs, hs, ys, ps = {}, {}, {}, {} # state of x, hidden, y, p(probability of y)\n    hs[-1] = np.copy(hprev) # init previous state of hidden/memory in dict {-1:hprev}\n    loss = 0\n    \n    # Forward\n    for t in range(len(inputs)): # idx of each word in input sentence / each time step\n        xs[t] = np.zeros((vocab_size,1)) \n        xs[t][inputs[t]] = 1 # t-th word's vector's t-th element = 1 \n        hs[t] = np.tanh(np.dot(Wxh,xs[t]) + np.dot(Whh,hs[t-1]) + bh) # ---错误更正---\n        ys[t] = np.dot(Why, hs[t])+by\n        ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t]))) # ps[t] 向量的每个元素对应词汇表中每个单词的概率\n        \n        loss += -np.log(ps[t][targets[t],0]) # ps[t][targets[t]]选择出label对应盖茨的概率, 0 ???\n        \n#         if t < 2:\n#             print('xs[t]: ',xs[t])\n#             print('hs[t]: ',hs[t])\n#             print('ys[t]: ',ys[t])\n#             print('ps[t]: ',ps[t])\n            \n    # Backward\n    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n    dhnext = np.zeros_like(hs[0]) # hs[3] also Ok, cuz each vector has same shape\n    for t in reversed(range(len(inputs))):\n        dy = np.copy(ps[t]) # back softmax-crossentropy\n        dy[targets[t]] -= 1\n        dWhy += np.dot(dy, hs[t].T)\n        dby += dy\n        dh = np.dot(Why.T, dy) + dhnext # if variable being inited above, use +=, else =\n        dhraw = (1-hs[t]*hs[t]) * dh \n        dbh += dhraw\n        dWhh += np.dot(dhraw, hs[t-1].T)\n        dWxh += np.dot(dhraw, xs[t].T)\n        dhnext += np.dot(Whh.T, dhraw)\n    \n    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n        np.clip(dparam, -5, 5, out=dparam) # eliminate gradient vanishing, exploding\n        \n    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1] \n    #  hs[len(inputs)-1]: last second hidden state/memory of this input sentence",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c7da1eb17b2657d74b210f1bdbb50caad978b543"
      },
      "cell_type": "markdown",
      "source": "- dict\n```python\na = {}\na[-1] = 3\na['-2']=1\n```\n```python\n{-1: 3, '-2': 1}\n```\n\n- reversed\n```python\nfor i in reversed(range(3)):\n    print(i)\n```\n```python\n2 1 0\n```\n\n- np.clip(a, a_min, a_max, out=None)\n```python\n\"\"\"\nSignature: np.clip(a, a_min, a_max, out=None)\nDocstring: Clip (limit) the values in an array.\n\"\"\"\na = np.array([i for i in range(10)])\n# array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nnp.clip(a, 3, 7, out=a) # output a \na # array([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])\n```"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16701864198f6f97e441c62168d8d6034befdf2b"
      },
      "cell_type": "code",
      "source": "def sample(h, seed_ix, n):\n    # h: last hidden state / memory\n    # seed_idx: the idx of the first word/char of the sentence we want to generate in corpus\n    # n: the length of the sentence we want to generate, how many characters to predict\n    \n    # create the first word's/char's vector\n    x = np.zeros((vocab_size,1))\n    x[seed_ix] = 1\n    ixes = [] # resotre the idx of words/chars of the sentence\n    \n    for t in range(n):\n        h = np.tanh(np.dot(Wxh,x) + np.dot(Whh, h) + bh )\n        y = np.dot(Why, h)+by\n        p = np.exp(y) / np.sum(np.exp(y))\n        # select the biggest element? NO!NO! select randomly\n        ix = np.random.choice(range(vocab_size), p=p.ravel())\n        x = np.zeros((vocab_size,1))\n        x[ix] = 1\n        ixes.append(ix)\n    txt = ''.join(idx_to_char[ix] for ix in ixes)\n    print('-----\\n',txt,'\\n-----')\n    \nhprev = np.zeros((hidden_size,1))\nsample(hprev,char_to_idx['a'],100)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-----\n CJgimPdYpg\n\" sPS\n?dDVyw!S,.lBUJMUIas\nok﻿;rghu(dYjh NthuO\nIyoi'WqWv\nEke(luUblwq;kNEPQgig)W?MxAFfb\"yUE \n-----\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c98bae1523bb5cba5ac3222a1e4a28b3501a414e"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69048582eade26f578fe2a9412c20a6786e8c741"
      },
      "cell_type": "markdown",
      "source": "- array.eval()\n\n```python\na = np.array([[1,2,3],[4,5,6]])\na_exp = np.exp(a) / np.sum(np.exp(a))\n=>\n# array([[0.00426978, 0.01160646, 0.03154963],\n#            [0.08576079, 0.23312201, 0.63369132]])\na_exp.ravel()\n=>\n# array([0.00426978, 0.01160646, 0.03154963, 0.08576079, 0.23312201,\n#        0.63369132])\n```\n- np.random.choice()\n```python\n# select a element randomly\nnp.random.choice(range(6),p=a_exp.ravel()) \n# p means probability, or else, ValueError: probabilities do not sum to 1\n```"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "549ddc9de7a57b1be706e16dc2968e47f165e550"
      },
      "cell_type": "code",
      "source": "p = 0\ninputs = [char_to_idx[i] for i in data[p:p+seq_length]]\ntargets = [char_to_idx[i] for i in data[p+1:p+1+seq_length]]\nprint('inputs: ',inputs,'\\ntargets: ',targets)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "inputs:  [62, 26, 48, 39, 1, 47, 49, 52, 48, 43, 48, 41, 7, 1, 57, 42, 39, 48, 1, 19, 52, 39, 41, 49, 52] \ntargets:  [26, 48, 39, 1, 47, 49, 52, 48, 43, 48, 41, 7, 1, 57, 42, 39, 48, 1, 19, 52, 39, 41, 49, 52, 1]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dd4a3c244e691d0f29ab794d8b1ec19d38327ca5"
      },
      "cell_type": "code",
      "source": "n,p = 0, 0\n# memory variables for Adagrad\nmWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\nmbh, mby = np.zeros_like(bh), np.zeros_like(by)\n\nsmooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                \nloss_list = []\nwhile n <= 1000 * 100000:\n    if p+1+seq_length >= len(data) or n ==0:\n        hprev = np.zeros((hidden_size,1))\n        p = 0\n        \n    inputs = [char_to_idx[ch] for ch in data[p:p+seq_length]]\n    targets = [char_to_idx[ch] for ch in data[p+1:p+seq_length+1]]\n\n    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFunc(inputs, targets, hprev)\n    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n    loss_list.append(smooth_loss)\n    \n    # sample from the model now and then                                                                                                                                                        \n    if n % 1000 == 0:\n        print('iter: {} - loss: {}'.format(n/1000, smooth_loss)) # print progress\n        sample(hprev, inputs[0], 200)\n\n    # perform parameter update with Adagrad                                                                                                                                                     \n    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n                                  [dWxh, dWhh, dWhy, dbh, dby],\n                                  [mWxh, mWhh, mWhy, mbh, mby]):\n        mem += dparam * dparam\n        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update       \n    \n    p += seq_length\n    n += 1\n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "iter: 0.0 - loss: 103.57836973586478\n-----\n (':yBrUqLMd;!﻿:O?)Ox,us)b LTFsaoN﻿Cc,.Qfhu ﻿NJraBpwe﻿J'p);eçOç\"hMEozFLYUtGPQs?uakePL:HMtdiIuWgssGYeyIE-,hçqJ﻿,UAjQ\"VIhYiLNt\nwviTxm\nx .ft\n)Sh LB﻿rGY?C..!ErB:iTm Bm\"fEGJvçov\"qlL,zCCUy:.xUe OB'mu;yU,Yh'M \n-----\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "486a3f759bf866ace785e1be899e4fd5000f25c3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0498b9ea03fd9c25f6c304c60f771e1a1951b388"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e3f51b25c2910e282a328865f4a22f98ae054ad"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2beac2a075acc494a1b1ec0a018dda47f231de5"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3f6fa7540f3a27fddb62ea450c36f7705e3aeeb"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5043a3dd4d6af22812f12812a4a876fc13a11651"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db53bd2ba450a6e1485343f4de3aed173aac8c31"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}