{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4323a1cc73e0634f4c3938a147fa6f321d9cc82"
      },
      "cell_type": "code",
      "source": "import tensorflow as tf\nimport numpy as np\nimport collections\nimport datetime as dt",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb4a3be6d5da3390b37eace0d3ba0ebfb8e0c74c"
      },
      "cell_type": "code",
      "source": "import os\nimport argparse\nimport sys\nfrom tempfile import gettempdir\nfrom six.moves import urllib\n\ncurrent_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    '--log_dir',\n    type=str,\n    default=os.path.join(current_path, 'log'),\n    help='The log directory for TensorBoard summaries.')\nFLAGS, unparsed = parser.parse_known_args()\n\n# Create the directory for TensorBoard variables if there is not.\nif not os.path.exists(FLAGS.log_dir):\n    os.makedirs(FLAGS.log_dir)\n\n# Step 1: Download the data.\nurl = ' http://www.fit.vutbr.cz/~imikolov/rnnlm/'\n\n# pylint: disable=redefined-outer-name\ndef maybe_download(filename, expected_bytes):\n    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n    local_filename = os.path.join(gettempdir(), filename)\n    if not os.path.exists(local_filename):\n        local_filename, _ = urllib.request.urlretrieve(url + filename,\n                                                   local_filename)\n    statinfo = os.stat(local_filename)\n    if statinfo.st_size == expected_bytes:\n        print('Found and verified', filename)\n    else:\n        print(statinfo.st_size)\n        raise Exception('Failed to verify ' + local_filename +\n                    '. Can you get to it with a browser?')\n    return local_filename\n\n\ntgz_filename = maybe_download('simple-examples.tgz', 34869662)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found and verified simple-examples.tgz\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8be38a21f9de9b9fc3ec5f2dc6a881740cc06ec7"
      },
      "cell_type": "code",
      "source": "tgz_filename",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "'/tmp/simple-examples.tgz'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "27b745ebe16e085cfbbadc5101f7e11608b0e355"
      },
      "cell_type": "code",
      "source": "os.listdir('/tmp')",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "['.keras', 'simple-examples.tgz', '.cache', '.config', '.ipython', '.local']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6553b2b445f26cb2952e6655b9035bcc1f126e97"
      },
      "cell_type": "code",
      "source": "import tarfile\ntarobj = tarfile.open(tgz_filename, \"r:gz\")\nfor tarinfo in tarobj:\n    tarobj.extract(tarinfo.name, r\"/tmp\")\ntarobj.close()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48f440b4a0a04553c215bc1ad18b00aae9ef592d"
      },
      "cell_type": "code",
      "source": "os.listdir('/tmp')",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "['.keras',\n 'simple-examples.tgz',\n '.cache',\n 'simple-examples',\n '.config',\n '.ipython',\n '.local']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bdb2e77764b8a1b2ad476fb81ac543c47650205"
      },
      "cell_type": "code",
      "source": "os.listdir('/tmp/simple-examples/data')",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "['ptb.char.train.txt',\n 'ptb.test.txt',\n 'ptb.valid.txt',\n 'README',\n 'ptb.char.valid.txt',\n 'ptb.char.test.txt',\n 'ptb.train.txt']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "data_path = '/tmp/simple-examples/data'\n\ndef read_words(filename): \n    # filename is a .txt file\n    with tf.gfile.GFile(filename, \"r\") as f:\n        return f.read().replace(\"\\n\", \"<eos>\").split()\n        # f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n\n\ndef build_vocab(filename):\n    data = read_words(filename)\n\n    counter = collections.Counter(data)\n    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n\n    words, _ = list(zip(*count_pairs))\n    word_to_id = dict(zip(words, range(len(words))))\n\n    return word_to_id\n\n\ndef file_to_word_ids(filename, word_to_id):\n    data = read_words(filename)\n    return [word_to_id[word] for word in data if word in word_to_id]\n\n\ndef load_data():\n    # get the data paths, which stores the un-zipped .txt file\n    data_path = '/tmp/simple-examples/data'\n    train_path = os.path.join(data_path, \"ptb.train.txt\")\n    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n    test_path = os.path.join(data_path, \"ptb.test.txt\")\n\n    # build the complete vocabulary, then convert text data to list of integers\n    word_to_id = build_vocab(train_path)\n    train_data = file_to_word_ids(train_path, word_to_id)\n    valid_data = file_to_word_ids(valid_path, word_to_id)\n    test_data = file_to_word_ids(test_path, word_to_id)\n    vocabulary = len(word_to_id)\n    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n\n    print(train_data[:5])\n    print(word_to_id)\n    print(vocabulary)\n    print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n    return train_data, valid_data, test_data, vocabulary, reversed_dictionary\n\n\ndef batch_producer(raw_data, batch_size, num_steps):\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0: batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    x = data[:, i * num_steps:(i + 1) * num_steps]\n    x.set_shape([batch_size, num_steps])\n    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n    y.set_shape([batch_size, num_steps])\n    return x, y\n\n\nclass Input(object):\n    def __init__(self, batch_size, num_steps, data):\n        self.batch_size = batch_size\n        self.num_steps = num_steps\n        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)\n\n\n# create the main model\nclass Model(object):\n    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,\n                 dropout=0.5, init_scale=0.05):\n        self.is_training = is_training\n        self.input_obj = input\n        self.batch_size = input.batch_size\n        self.num_steps = input.num_steps\n        self.hidden_size = hidden_size\n\n        # create the word embeddings\n        with tf.device(\"/cpu:0\"):\n            embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n\n        if is_training and dropout < 1:\n            inputs = tf.nn.dropout(inputs, dropout)\n\n        # set up the state storage / extraction\n        self.init_state = tf.placeholder(tf.float32, [num_layers, 2, self.batch_size, self.hidden_size])\n\n        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n        rnn_tuple_state = tuple(\n            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n             for idx in range(num_layers)]\n        )\n\n        # create an LSTM cell to be unrolled\n        cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias=1.0)\n        # add a dropout wrapper if training\n        if is_training and dropout < 1:\n            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n        if num_layers > 1:\n            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\n\n        output, self.state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32, initial_state=rnn_tuple_state)\n        # reshape to (batch_size * num_steps, hidden_size)\n        output = tf.reshape(output, [-1, hidden_size])\n\n        softmax_w = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -init_scale, init_scale))\n        softmax_b = tf.Variable(tf.random_uniform([vocab_size], -init_scale, init_scale))\n        logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n        # Reshape logits to be a 3-D tensor for sequence loss\n        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n\n        # Use the contrib sequence loss and average over the batches\n        loss = tf.contrib.seq2seq.sequence_loss(\n            logits,\n            self.input_obj.targets,\n            tf.ones([self.batch_size, self.num_steps], dtype=tf.float32),\n            average_across_timesteps=False,\n            average_across_batch=True)\n\n        # Update the cost\n        self.cost = tf.reduce_sum(loss)\n\n        # get the prediction accuracy\n        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        if not is_training:\n            return\n        self.learning_rate = tf.Variable(0.0, trainable=False)\n\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 5)\n        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n        # optimizer = tf.train.AdamOptimizer(self.learning_rate)\n        self.train_op = optimizer.apply_gradients(\n            zip(grads, tvars),\n            global_step=tf.contrib.framework.get_or_create_global_step())\n        # self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cost)\n\n        self.new_lr = tf.placeholder(tf.float32, shape=[])\n        self.lr_update = tf.assign(self.learning_rate, self.new_lr)\n\n    def assign_lr(self, session, lr_value):\n        session.run(self.lr_update, feed_dict={self.new_lr: lr_value})\n\n\ndef train(train_data, vocabulary, num_layers, num_epochs, batch_size, model_save_name,\n          learning_rate=1.0, max_lr_epoch=10, lr_decay=0.93, print_iter=50):\n    # setup data and models\n    training_input = Input(batch_size=batch_size, num_steps=35, data=train_data)\n    m = Model(training_input, is_training=True, hidden_size=650, vocab_size=vocabulary,\n              num_layers=num_layers)\n    init_op = tf.global_variables_initializer()\n    orig_decay = lr_decay\n    with tf.Session() as sess:\n        # start threads\n        sess.run([init_op])\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n        saver = tf.train.Saver()\n        for epoch in range(num_epochs):\n            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n            m.assign_lr(sess, learning_rate * new_lr_decay)\n            # m.assign_lr(sess, learning_rate)\n            # print(m.learning_rate.eval(), new_lr_decay)\n            current_state = np.zeros((num_layers, 2, batch_size, m.hidden_size))\n            curr_time = dt.datetime.now()\n            for step in range(training_input.epoch_size):\n                # cost, _ = sess.run([m.cost, m.optimizer])\n                if step % print_iter != 0:\n                    cost, _, current_state = sess.run([m.cost, m.train_op, m.state],\n                                                      feed_dict={m.init_state: current_state})\n                else:\n                    seconds = (float((dt.datetime.now() - curr_time).seconds) / print_iter)\n                    curr_time = dt.datetime.now()\n                    cost, _, current_state, acc = sess.run([m.cost, m.train_op, m.state, m.accuracy],\n                                                           feed_dict={m.init_state: current_state})\n                    print(\"Epoch {}, Step {}, cost: {:.3f}, accuracy: {:.3f}, Seconds per step: {:.3f}\".format(epoch,\n                            step, cost, acc, seconds))\n\n            # save a model checkpoint\n            saver.save(sess, data_path + '\\\\' + model_save_name, global_step=epoch)\n        # do a final save\n        saver.save(sess, data_path + '\\\\' + model_save_name + '-final')\n        # close threads\n        coord.request_stop()\n        coord.join(threads)\n\n\ndef test(model_path, test_data, reversed_dictionary):\n    test_input = Input(batch_size=20, num_steps=35, data=test_data)\n    m = Model(test_input, is_training=False, hidden_size=650, vocab_size=vocabulary,\n              num_layers=2)\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        # start threads\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n        current_state = np.zeros((2, 2, m.batch_size, m.hidden_size))\n        # restore the trained model\n        saver.restore(sess, model_path)\n        # get an average accuracy over num_acc_batches\n        num_acc_batches = 30\n        check_batch_idx = 25\n        acc_check_thresh = 5\n        accuracy = 0\n        for batch in range(num_acc_batches):\n            if batch == check_batch_idx:\n                true_vals, pred, current_state, acc = sess.run([m.input_obj.targets, m.predict, m.state, m.accuracy],\n                                                               feed_dict={m.init_state: current_state})\n                pred_string = [reversed_dictionary[x] for x in pred[:m.num_steps]]\n                true_vals_string = [reversed_dictionary[x] for x in true_vals[0]]\n                print(\"True values (1st line) vs predicted values (2nd line):\")\n                print(\" \".join(true_vals_string))\n                print(\" \".join(pred_string))\n            else:\n                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={m.init_state: current_state})\n            if batch >= acc_check_thresh:\n                accuracy += acc\n        print(\"Average accuracy: {:.3f}\".format(accuracy / (num_acc_batches-acc_check_thresh)))\n        # close threads\n        coord.request_stop()\n        coord.join(threads)\n",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# if args.data_path:\n#     data_path = args.data_path\ntrain_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data()\n# if args.run_opt == 1:\ntrain(train_data, vocabulary, num_layers=2, num_epochs=60, batch_size=20,\n      model_save_name='two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr')\n# else:\n#     trained_model = args.data_path + \"\\\\two-layer-lstm-medium-config-60-epoch-0p93-lr-decay-10-max-lr-38\"\n#     test(trained_model, test_data, reversed_dictionary)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75b1236b6563ccf7e7f7a4b5cd827d98e64cff23"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}